{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b92db05",
   "metadata": {},
   "source": [
    "# üìò **Notes on the Law of Large Numbers (LLN)**\n",
    "\n",
    "## üéØ **What the LLN Says (Informal Statement)**\n",
    "When you take the average of many independent, identically distributed (i.i.d.) random variables, that average will get closer and closer to the true mean of the distribution as the number of samples increases.\n",
    "\n",
    "In plain language:  \n",
    "**More data ‚Üí better estimate of the true mean.**\n",
    "\n",
    "---\n",
    "\n",
    "# üß© **Setup**\n",
    "Let  \n",
    "- $X_1, X_2, \\dots, X_n$ be **i.i.d.** random variables  \n",
    "- Each with mean $ \\mu $ and variance $ \\sigma^2 $\n",
    "\n",
    "Define the **sample mean**:\n",
    "$$\n",
    "\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "# üìå **Two Versions of the LLN**\n",
    "\n",
    "## 1. **Weak Law of Large Numbers (WLLN)**\n",
    "$$\n",
    "\\bar{X}_n \\xrightarrow{P} \\mu\n",
    "$$\n",
    "\n",
    "Meaning:  \n",
    "For any $ \\varepsilon > 0 $,\n",
    "$$\n",
    "P(|\\bar{X}_n - \\mu| > \\varepsilon) \\to 0 \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "The probability of the sample mean $\\bar{X}_n$ being more than $\\varepsilon$ away from the actual mean $\\mu$, goes to 0 as n goes to infinity. This is **convergence in probability**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. **Strong Law of Large Numbers (SLLN)**\n",
    "$$\n",
    "\\bar{X}_n \\xrightarrow{\\text{a.s.}} \\mu\n",
    "$$\n",
    "\n",
    "Meaning:  \n",
    "$$\n",
    "P\\left( \\lim_{n\\to\\infty} \\bar{X}_n = \\mu \\right) = 1.\n",
    "$$\n",
    "\n",
    "This is **almost sure convergence**, a stronger guarantee.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† **Why the LLN Works (Key Ideas)**\n",
    "\n",
    "### ‚úî Expectation of the sample mean:\n",
    "$$\n",
    "\\mathbb{E}[\\bar{X}_n] = \\mu\n",
    "$$\n",
    "\n",
    "### ‚úî Variance of the sample mean:\n",
    "$$\n",
    "\\mathrm{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "As $ n $ grows, the variance shrinks, meaning the distribution of $ \\bar{X}_n $ becomes more concentrated around $ \\mu $.\n",
    "\n",
    "### ‚úî Chebyshev‚Äôs inequality gives the WLLN:\n",
    "$$\n",
    "P(|\\bar{X}_n - \\mu| \\ge \\varepsilon)\n",
    "\\le \\frac{\\sigma^2}{n\\varepsilon^2}\n",
    "\\to 0.\n",
    "$$\n",
    "\n",
    "This is the classic proof route.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä **Intuition**\n",
    "- Each sample is noisy.  \n",
    "- Averaging reduces noise because independent fluctuations cancel out.  \n",
    "- The ‚Äúsignal‚Äù (the mean) stays the same, but the ‚Äúnoise‚Äù (variance) shrinks like $1/n$.\n",
    "\n",
    "This is why:\n",
    "- Polls work  \n",
    "- Monte Carlo simulations converge  \n",
    "- Casinos make money  \n",
    "- Sample averages stabilize with large datasets  \n",
    "\n",
    "---\n",
    "\n",
    "# üß™ **Examples**\n",
    "\n",
    "### **Coin flips**\n",
    "Let $X_i = 1$ for heads, $0$ for tails.  \n",
    "Then $ \\mu = p $, the probability of heads.\n",
    "\n",
    "LLN says:\n",
    "$$\n",
    "\\frac{\\text{\\# of heads}}{n} \\to p.\n",
    "$$\n",
    "\n",
    "### **Rolling a die**\n",
    "Mean of a fair die is $3.5$.  \n",
    "LLN says the average of many rolls approaches 3.5.\n",
    "\n",
    "---\n",
    "\n",
    "# üîç **LLN vs. Central Limit Theorem (CLT)**\n",
    "\n",
    "| Concept | LLN | CLT |\n",
    "|--------|-----|-----|\n",
    "| What it describes | Convergence of the sample mean to the true mean | Shape of the distribution of the sample mean |\n",
    "| Type of convergence | Probability / almost sure | Distributional |\n",
    "| Result | $ \\bar{X}_n \\to \\mu $ | $ \\sqrt{n}(\\bar{X}_n - \\mu) \\to \\mathcal{N}(0,\\sigma^2) $ |\n",
    "\n",
    "LLN = **where the mean goes**  \n",
    "CLT = **how it fluctuates around the mean**\n",
    "\n",
    "---\n",
    "\n",
    "# üìù **Key Takeaways**\n",
    "- LLN guarantees that sample averages stabilize around the true mean.  \n",
    "- Works for any distribution with finite mean and variance.  \n",
    "- Independence is crucial.  \n",
    "- Variance shrinking like $1/n$ is the engine behind the theorem.  \n",
    "- Foundation for statistics, machine learning, and simulation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

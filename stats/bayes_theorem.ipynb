{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b11de5d",
   "metadata": {},
   "source": [
    "# âœ… Bayes Theorem - The Fundamental Definitions\n",
    "\n",
    "Bayesâ€™ theorem is essentially a **reâ€‘expression of conditional probability**, and conditional probability itself is built directly from **joint** and **marginal** probabilities.\n",
    "\n",
    "Letâ€™s break it down.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Step 1: Start with the definition of conditional probability  \n",
    "By definition:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "- The **numerator** is a **joint probability**: the chance that **A and B happen together**.\n",
    "- The **denominator** is a **marginal probability**: the overall chance that **B happens**, regardless of A.\n",
    "\n",
    "This definition is the seed from which Bayesâ€™ theorem grows.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Step 2: Write the symmetric version  \n",
    "We can also write:\n",
    "\n",
    "$$\n",
    "P(B \\mid A) = \\frac{P(A \\cap B)}{P(A)}\n",
    "$$\n",
    "\n",
    "Both expressions contain the same joint probability $P(A \\cap B)$.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Step 3: Solve for the joint probability  \n",
    "From the first equation:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(A \\mid B) \\cdot P(B)\n",
    "$$\n",
    "\n",
    "From the second:\n",
    "\n",
    "$$\n",
    "P(A \\cap B) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Set them equal:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) \\cdot P(B) = P(B \\mid A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§© Step 4: Rearrange â€” and Bayesâ€™ theorem appears  \n",
    "Solve for $P(A \\mid B)$:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A)\\,P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This is **Bayesâ€™ theorem**.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ How Joint and Marginal Probabilities Fit In\n",
    "\n",
    "| Component | Role in Bayesâ€™ Theorem | Interpretation |\n",
    "|----------|-------------------------|----------------|\n",
    "| **Joint probability** $P(A \\cap B)$ | Hidden inside the numerator | â€œHow often do A and B occur together?â€ |\n",
    "| **Marginal probability** $P(B)$ | Denominator | â€œHow common is B overall?â€ |\n",
    "| **Conditional probability** $P(B \\mid A)$ | Likelihood term | â€œIf A happens, how likely is B?â€ |\n",
    "| **Marginal probability** $P(A)$ | Prior | â€œHow common is A before seeing B?â€ |\n",
    "\n",
    "Bayesâ€™ theorem is really just a clever way of **rewriting joint probability in two different ways** and then solving for the conditional you want.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸŒ± Intuition in one sentence  \n",
    "**Bayesâ€™ theorem updates your belief about A after observing B by comparing how strongly A predicts B (likelihood) against how common B is overall (marginal).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35161763",
   "metadata": {},
   "source": [
    "# ğŸ¯ The Denominator Rephrased\n",
    "\n",
    "When you see the denominator $P(B)$ written as a **sum of two pieces**, itâ€™s because weâ€™re using the **Law of Total Probability**.\n",
    "\n",
    "Suppose youâ€™re conditioning on event $A$. Then the event $B$ can happen in two mutually exclusive ways:\n",
    "\n",
    "1. **B happens while A is true**  \n",
    "2. **B happens while A is false** (i.e., $A^c$)\n",
    "\n",
    "So we decompose:\n",
    "\n",
    "$$\n",
    "P(B) = P(B \\cap A) + P(B \\cap A^c)\n",
    "$$\n",
    "\n",
    "Now substitute the conditional-probability definitions:\n",
    "\n",
    "$$\n",
    "P(B) = P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)\n",
    "$$\n",
    "\n",
    "These are the â€œtwo piecesâ€ youâ€™re referring to.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ” Why This Matters for Bayesâ€™ Theorem\n",
    "\n",
    "Bayesâ€™ theorem is:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "But since $P(B)$ is often hard to compute directly, we replace it with the total-probability expansion:\n",
    "\n",
    "$$\n",
    "P(A \\mid B)\n",
    "= \\frac{P(B \\mid A)P(A)}\n",
    "       {P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}\n",
    "$$\n",
    "\n",
    "This is the version you see in medical-test examples, spam filters, and most real-world applications.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸŒŸ Intuition\n",
    "\n",
    "Think of $B$ as â€œa positive test result.â€\n",
    "\n",
    "There are two ways to test positive:\n",
    "\n",
    "- You **have** the condition and the test detects it  \n",
    "- You **donâ€™t have** the condition but the test gives a false positive  \n",
    "\n",
    "Those are exactly the two pieces:\n",
    "\n",
    "- True positives: $P(B \\mid A)P(A)$\n",
    "- False positives: $P(B \\mid A^c)P(A^c)$\n",
    "\n",
    "Add them together and you get the total probability of a positive test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42c266",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¯ The Core Idea: â€œPosterior becomes the next priorâ€\n",
    "\n",
    "Bayesian updating is a **recursive** process. Each time you observe new evidence, you update your belief. That updated belief becomes the starting point for the next update.\n",
    "\n",
    "Formally:\n",
    "\n",
    "- Start with a **prior** $P(A)$\n",
    "- Observe evidence $B$\n",
    "- Compute the **posterior**:\n",
    "\n",
    "$$\n",
    "P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- Then, when new evidence $C$ arrives, you treat the posterior as your new prior:\n",
    "\n",
    "$$\n",
    "\\text{New prior} = P(A \\mid B)\n",
    "$$\n",
    "\n",
    "and update again:\n",
    "\n",
    "$$\n",
    "P(A \\mid B, C) = \\frac{P(C \\mid A, B)\\, P(A \\mid B)}{P(C \\mid B)}\n",
    "$$\n",
    "\n",
    "This is the entire Bayesian engine.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ” Why this works: Beliefs accumulate evidence\n",
    "\n",
    "Bayesian inference is like a running tally:\n",
    "\n",
    "- You start with an initial belief (prior)\n",
    "- Each piece of evidence nudges that belief up or down\n",
    "- The updated belief becomes your new baseline\n",
    "\n",
    "This mirrors how humans intuitively learn: every new observation modifies what we think is likely.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“¦ A Simple Example: Is a coin biased?\n",
    "\n",
    "Suppose youâ€™re unsure whether a coin is fair. Let:\n",
    "\n",
    "- $A$: â€œthe coin is biased toward headsâ€\n",
    "- Your initial prior: $P(A) = 0.5$\n",
    "\n",
    "### Step 1 â€” You flip the coin and see **heads**.\n",
    "\n",
    "Use Bayesâ€™ theorem to compute:\n",
    "\n",
    "$$\n",
    "P(A \\mid \\text{heads})\n",
    "$$\n",
    "\n",
    "This gives you a posterior â€” your updated belief about bias.\n",
    "\n",
    "### Step 2 â€” You flip again and see **heads**.\n",
    "\n",
    "Now your **new prior** is:\n",
    "\n",
    "$$\n",
    "P(A)_{\\text{new}} = P(A \\mid \\text{heads from step 1})\n",
    "$$\n",
    "\n",
    "You update again:\n",
    "\n",
    "$$\n",
    "P(A \\mid \\text{heads, heads})\n",
    "$$\n",
    "\n",
    "### Step 3 â€” Another flip, more updatingâ€¦\n",
    "\n",
    "Each time:\n",
    "\n",
    "$$\n",
    "\\text{posterior} \\rightarrow \\text{next prior}\n",
    "$$\n",
    "\n",
    "This is how Bayesian inference â€œlearnsâ€ from data.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Intuition: Bayesian updating is like compounding interest\n",
    "\n",
    "Each piece of evidence compounds your belief.\n",
    "\n",
    "- If evidence consistently supports $A$, your posterior grows rapidly.\n",
    "- If evidence contradicts $A$, your posterior shrinks.\n",
    "- If evidence is ambiguous, your belief moves only slightly.\n",
    "\n",
    "The recursive structure is what makes Bayesian methods so powerful in statistics, machine learning, and decision-making."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15d10945",
   "metadata": {},
   "source": [
    "# üìò Chebyshev‚Äôs Inequality ‚Äî Study Notes\n",
    "\n",
    "[Steve Brunton: Chebyshev's Inequality in Probability: Second Order Estimates](https://www.youtube.com/watch?v=otCHN3s52ho&list=PLMrJAkhIeNNR3sNYvfgiKgcStwuPSts9V&index=35)\n",
    "\n",
    "## üéØ Core Idea\n",
    "Chebyshev‚Äôs Inequality gives a universal bound on **how much probability mass can lie far from the mean**, using only the **variance**.  \n",
    "It works for *any* random variable with a finite mean and variance ‚Äî no assumptions about shape, symmetry, or tails.\n",
    "\n",
    "---\n",
    "\n",
    "# üìê The Statement\n",
    "\n",
    "For any random variable $X$ with mean $\\mu$ and variance $\\sigma^2$, and for any number $a > 0$:\n",
    "\n",
    "$$\n",
    "\\Pr\\left(|X - \\mu| \\ge a\\right) \\le \\frac{\\sigma^2}{a^2}.\n",
    "$$\n",
    "\n",
    "This means:  \n",
    "> The probability that $X$ deviates from its mean by at least $a$ is at most $\\sigma^2 / a^2$.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Intuition\n",
    "\n",
    "### Why does this make sense?\n",
    "- Variance measures **average squared deviation** from the mean.\n",
    "- If variance is small, the distribution must be tightly clustered.\n",
    "- If variance is large, the distribution can spread out more.\n",
    "- Chebyshev formalizes this:  \n",
    "  > Large deviations are rare unless the variance is large.\n",
    "\n",
    "### Visual intuition\n",
    "Imagine a bell curve, a uniform distribution, or even a weird multimodal shape.  \n",
    "Regardless of shape, if the variance is fixed, you **cannot** push too much probability mass far from the mean ‚Äî otherwise the average squared deviation would blow up.\n",
    "\n",
    "---\n",
    "\n",
    "# üîß How It‚Äôs Proven (High-Level)\n",
    "\n",
    "The proof uses **Markov‚Äôs Inequality**, which applies to non‚Äënegative random variables.\n",
    "\n",
    "1. Define a new variable  \n",
    "   $$\n",
    "   Y = (X - \\mu)^2.\n",
    "   $$\n",
    "   This is always non‚Äënegative.\n",
    "\n",
    "2. Notice that  \n",
    "   $$\n",
    "   \\Pr(|X - \\mu| \\ge a) = \\Pr(Y \\ge a^2).\n",
    "   $$\n",
    "\n",
    "3. Apply Markov‚Äôs Inequality to $Y$:  \n",
    "   $$\n",
    "   \\Pr(Y \\ge a^2) \\le \\frac{\\mathbb{E}[Y]}{a^2}.\n",
    "   $$\n",
    "\n",
    "4. But $\\mathbb{E}[Y] = \\sigma^2$, the variance.\n",
    "\n",
    "5. Therefore:  \n",
    "   $$\n",
    "   \\Pr(|X - \\mu| \\ge a) \\le \\frac{\\sigma^2}{a^2}.\n",
    "   $$\n",
    "\n",
    "That‚Äôs the entire proof ‚Äî elegant and surprisingly simple.\n",
    "\n",
    "---\n",
    "\n",
    "# üìä Practical Interpretations\n",
    "\n",
    "### In terms of standard deviations\n",
    "Let $a = k\\sigma$. Then:\n",
    "\n",
    "$$\n",
    "\\Pr(|X - \\mu| \\ge k\\sigma) \\le \\frac{1}{k^2}.\n",
    "$$\n",
    "\n",
    "Examples:\n",
    "- $k = 2$: at most $1/4 = 25\\%$ of values lie ‚â• 2œÉ from the mean  \n",
    "- $k = 3$: at most $1/9 \\approx 11.1\\%$ lie ‚â• 3œÉ  \n",
    "- $k = 10$: at most $1\\%$ lie ‚â• 10œÉ\n",
    "\n",
    "These bounds are often loose, but they work for *every* distribution.\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Why It Matters\n",
    "\n",
    "### 1. **Distribution-Free Guarantee**\n",
    "You don‚Äôt need normality, symmetry, or anything fancy.  \n",
    "If you know the variance, you get a bound.\n",
    "\n",
    "### 2. **Foundation for the Law of Large Numbers**\n",
    "Chebyshev is the key tool for proving that sample averages converge to the true mean.\n",
    "\n",
    "### 3. **Worst-Case Analysis**\n",
    "In statistics, machine learning, and probability theory, Chebyshev gives a safe upper bound when you know almost nothing about the distribution.\n",
    "\n",
    "---\n",
    "\n",
    "# üìù Example\n",
    "\n",
    "Suppose a random variable has:\n",
    "- mean $ \\mu = 10 $\n",
    "- variance $ \\sigma^2 = 4 $\n",
    "\n",
    "What‚Äôs the maximum probability that $X$ differs from the mean by 5 or more?\n",
    "\n",
    "$$\n",
    "\\Pr(|X - 10| \\ge 5) \\le \\frac{4}{25} = 0.16.\n",
    "$$\n",
    "\n",
    "So no matter what the distribution looks like, **at most 16%** of the probability mass can lie 5 units or more from the mean.\n",
    "\n",
    "---\n",
    "\n",
    "# üß≠ When to Use Chebyshev\n",
    "\n",
    "Use it when:\n",
    "- You know the mean and variance.\n",
    "- You don‚Äôt know the distribution shape.\n",
    "- You need a guaranteed upper bound.\n",
    "- You‚Äôre proving convergence or bounding tail probabilities.\n",
    "\n",
    "Avoid it when:\n",
    "- You have a normal distribution ‚Äî tighter bounds exist.\n",
    "- You need precise tail probabilities ‚Äî Chebyshev is conservative."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
